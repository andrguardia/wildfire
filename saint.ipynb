{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989ac1c3",
   "metadata": {},
   "source": [
    "# SAINT Transformer Architecture for wildfire prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf8584",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "In this section we will install all the dependencies needed to run the code, load the dataset, and import all the necessary libraries required to run the SAINT transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96a3174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: 2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install --upgrade torch pytorch_lightning scikit-learn pandas numpy<2 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc4200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1129582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  system:index  EVH   EVT    NDVI  PRES_max  SPFH_max    TMP_max  WIND_max  \\\n",
      "0        0_0_0   75  3092  5691.0   97165.0  0.006520  17.850000      4.88   \n",
      "1        0_1_0   89  3900  6176.0   98114.0  0.006578  21.589990      4.59   \n",
      "2        0_2_0   96  3097  6563.0   97881.0  0.007021  19.989984      3.85   \n",
      "3        0_3_0   21  3296  4034.0   98933.0  0.006326  22.369989      4.08   \n",
      "4        0_4_0   99  3097  8027.0   97243.0  0.007369  16.290002      5.66   \n",
      "\n",
      "   burned        date  elevation  sm_profile  sm_profile_wetness  sm_rootzone  \\\n",
      "0       0  2023-04-01  348.75693    0.365752            0.797881     0.389533   \n",
      "1       0  2023-04-01  363.53400    0.356276            0.777190     0.380164   \n",
      "2       0  2023-04-01  409.15347    0.361830            0.789306     0.381324   \n",
      "3       0  2023-04-01  263.18298    0.358382            0.781829     0.382392   \n",
      "4       0  2023-04-01  323.89856    0.367979            0.802734     0.387794   \n",
      "\n",
      "   sm_rootzone_wetness  sm_surface  sm_surface_wetness  \\\n",
      "0             0.849741    0.349155            0.761735   \n",
      "1             0.829294    0.351032            0.765764   \n",
      "2             0.831857    0.352863            0.769822   \n",
      "3             0.834207    0.354602            0.773576   \n",
      "4             0.845987    0.356098            0.776841   \n",
      "\n",
      "                                                .geo  \n",
      "0  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...  \n",
      "1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...  \n",
      "2  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...  \n",
      "3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...  \n",
      "4  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e96cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Handle missing values\n",
    "df.ffill(inplace=True)\n",
    "\n",
    "# Clean up column names (remove leading/trailing spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Encode categorical features\n",
    "cat_cols = ['EVT']\n",
    "\n",
    "# Encode numerical features\n",
    "num_cols = [\n",
    "    'EVH', 'NDVI', 'PRES_max', 'SPFH_max', 'TMP_max', 'WIND_max',\n",
    "    'elevation', 'sm_profile', 'sm_profile_wetness', 'sm_rootzone',\n",
    "    'sm_rootzone_wetness', 'sm_surface', 'sm_surface_wetness'\n",
    "]\n",
    "\n",
    "# Check and process 'date' column\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    num_cols += ['year', 'month', 'day']\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Ensure 'burned' is integer\n",
    "df['burned'] = df['burned'].astype(int)\n",
    "\n",
    "# Extract longitude and latitude from '.geo' JSON string\n",
    "def extract_coords(geo_str):\n",
    "    geo = json.loads(geo_str)\n",
    "    return pd.Series(geo['coordinates'], index=['longitude', 'latitude'])\n",
    "\n",
    "if '.geo' in df.columns:\n",
    "    df[['longitude', 'latitude']] = df['.geo'].apply(extract_coords)\n",
    "    num_cols += ['longitude', 'latitude']\n",
    "    df.drop(columns=['.geo'], inplace=True)\n",
    "\n",
    "target = 'burned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5345049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system:index</th>\n",
       "      <th>EVH</th>\n",
       "      <th>EVT</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>PRES_max</th>\n",
       "      <th>SPFH_max</th>\n",
       "      <th>TMP_max</th>\n",
       "      <th>WIND_max</th>\n",
       "      <th>burned</th>\n",
       "      <th>elevation</th>\n",
       "      <th>...</th>\n",
       "      <th>sm_profile_wetness</th>\n",
       "      <th>sm_rootzone</th>\n",
       "      <th>sm_rootzone_wetness</th>\n",
       "      <th>sm_surface</th>\n",
       "      <th>sm_surface_wetness</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0_0</td>\n",
       "      <td>75</td>\n",
       "      <td>3092</td>\n",
       "      <td>5691.0</td>\n",
       "      <td>97165.0</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0</td>\n",
       "      <td>348.75693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797881</td>\n",
       "      <td>0.389533</td>\n",
       "      <td>0.849741</td>\n",
       "      <td>0.349155</td>\n",
       "      <td>0.761735</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-118.815671</td>\n",
       "      <td>34.230304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_1_0</td>\n",
       "      <td>89</td>\n",
       "      <td>3900</td>\n",
       "      <td>6176.0</td>\n",
       "      <td>98114.0</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>21.589990</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0</td>\n",
       "      <td>363.53400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777190</td>\n",
       "      <td>0.380164</td>\n",
       "      <td>0.829294</td>\n",
       "      <td>0.351032</td>\n",
       "      <td>0.765764</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-118.546176</td>\n",
       "      <td>34.140472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_2_0</td>\n",
       "      <td>96</td>\n",
       "      <td>3097</td>\n",
       "      <td>6563.0</td>\n",
       "      <td>97881.0</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>19.989984</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0</td>\n",
       "      <td>409.15347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789306</td>\n",
       "      <td>0.381324</td>\n",
       "      <td>0.831857</td>\n",
       "      <td>0.352863</td>\n",
       "      <td>0.769822</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-118.582109</td>\n",
       "      <td>34.122506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_3_0</td>\n",
       "      <td>21</td>\n",
       "      <td>3296</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>98933.0</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>22.369989</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>263.18298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.382392</td>\n",
       "      <td>0.834207</td>\n",
       "      <td>0.354602</td>\n",
       "      <td>0.773576</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-118.627025</td>\n",
       "      <td>34.176405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_4_0</td>\n",
       "      <td>99</td>\n",
       "      <td>3097</td>\n",
       "      <td>8027.0</td>\n",
       "      <td>97243.0</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>16.290002</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0</td>\n",
       "      <td>323.89856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.387794</td>\n",
       "      <td>0.845987</td>\n",
       "      <td>0.356098</td>\n",
       "      <td>0.776841</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-118.707873</td>\n",
       "      <td>34.059624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  system:index  EVH   EVT    NDVI  PRES_max  SPFH_max    TMP_max  WIND_max  \\\n",
       "0        0_0_0   75  3092  5691.0   97165.0  0.006520  17.850000      4.88   \n",
       "1        0_1_0   89  3900  6176.0   98114.0  0.006578  21.589990      4.59   \n",
       "2        0_2_0   96  3097  6563.0   97881.0  0.007021  19.989984      3.85   \n",
       "3        0_3_0   21  3296  4034.0   98933.0  0.006326  22.369989      4.08   \n",
       "4        0_4_0   99  3097  8027.0   97243.0  0.007369  16.290002      5.66   \n",
       "\n",
       "   burned  elevation  ...  sm_profile_wetness  sm_rootzone  \\\n",
       "0       0  348.75693  ...            0.797881     0.389533   \n",
       "1       0  363.53400  ...            0.777190     0.380164   \n",
       "2       0  409.15347  ...            0.789306     0.381324   \n",
       "3       0  263.18298  ...            0.781829     0.382392   \n",
       "4       0  323.89856  ...            0.802734     0.387794   \n",
       "\n",
       "   sm_rootzone_wetness  sm_surface  sm_surface_wetness  year  month  day  \\\n",
       "0             0.849741    0.349155            0.761735  2023      4    1   \n",
       "1             0.829294    0.351032            0.765764  2023      4    1   \n",
       "2             0.831857    0.352863            0.769822  2023      4    1   \n",
       "3             0.834207    0.354602            0.773576  2023      4    1   \n",
       "4             0.845987    0.356098            0.776841  2023      4    1   \n",
       "\n",
       "    longitude   latitude  \n",
       "0 -118.815671  34.230304  \n",
       "1 -118.546176  34.140472  \n",
       "2 -118.582109  34.122506  \n",
       "3 -118.627025  34.176405  \n",
       "4 -118.707873  34.059624  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b804344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cat_cols = list(cat_cols)\n",
    "num_cols = list(num_cols)\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    df[col] = df[col].astype(int)  # Ensure integer type for embeddings\n",
    "\n",
    "# Compute cat_dims after encoding\n",
    "cat_dims = [df[col].nunique() for col in cat_cols]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df['burned']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure categorical columns are first and in the same order as cat_cols\n",
    "all_cols = cat_cols + [col for col in X_train.columns if col not in cat_cols]\n",
    "X_train = X_train[all_cols]\n",
    "X_test = X_test[all_cols]\n",
    "\n",
    "# Double-check for out-of-range values in categorical columns\n",
    "for i, col in enumerate(cat_cols):\n",
    "    max_val = X_train[col].max()\n",
    "    assert max_val < cat_dims[i], f\"Column {col} has value {max_val} >= {cat_dims[i]}\"\n",
    "\n",
    "# Convert ONLY numerical columns to float32 for PyTorch compatibility\n",
    "for col in num_cols:\n",
    "    X_train[col] = X_train[col].astype(np.float32)\n",
    "    X_test[col] = X_test[col].astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c59e8b",
   "metadata": {},
   "source": [
    "## 🔧 Defining the SAINT Model Architecture\n",
    "\n",
    "We build a **Transformer model** that works on tabular data:\n",
    "\n",
    "- **Categorical features** are turned into embeddings (like word vectors).\n",
    "- **Numerical features** are projected into the same embedding space via a linear layer.\n",
    "- All features become **tokens** and go into a Transformer Encoder.\n",
    "\n",
    "### Inside the Transformer:\n",
    "\n",
    "- **Self-Attention (column-wise)**: learns relationships between features.\n",
    "- **Intersample Attention (row-wise)**: each row can attend to other rows in the batch — **unique to SAINT**.\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The transformer output goes through a simple classifier:\n",
    "\n",
    "- `Linear → ReLU → Dropout → Linear → Sigmoid (via BCEWithLogitsLoss)`\n",
    "\n",
    "### Class Imbalance Handling\n",
    "\n",
    "We use `BCEWithLogitsLoss` with a `pos_weight` parameter to account for the fact that wildfire events are **rare** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23083b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SAINT(pl.LightningModule):\n",
    "    def __init__(self, num_cont, cat_dims=[], embed_dim=32, \n",
    "                 num_heads=4, num_layers=3, dropout=0.1, pos_weight=None, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['pos_weight'])\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, embed_dim) for dim in cat_dims\n",
    "        ])\n",
    "        self.cont_proj = nn.Linear(num_cont, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=dropout,\n",
    "            batch_first=True, dim_feedforward=embed_dim*4\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        if pos_weight is not None:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        tokens = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            tokens.append(emb(x_cat[:, i]))\n",
    "        tokens.append(self.cont_proj(x_cont))\n",
    "        tokens = torch.stack(tokens, dim=1)\n",
    "        attn_output = self.transformer(tokens)\n",
    "        cls_token = attn_output.mean(dim=1)\n",
    "        return self.classifier(cls_token).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_cat, x_cont, y = batch\n",
    "        logits = self(x_cat, x_cont)\n",
    "        loss = self.loss_fn(logits, y.float())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23174917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class FireDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, cat_cols, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Defensive: ensure all column names are strings\n",
    "        self.cat_cols = [str(col) for col in list(self.cat_cols)]\n",
    "        self.X_train.columns = [str(col) for col in self.X_train.columns]\n",
    "        self.X_test.columns = [str(col) for col in self.X_test.columns]\n",
    "        cat_cols = self.cat_cols\n",
    "        cont_cols = [col for col in self.X_train.columns if col not in cat_cols]\n",
    "\n",
    "        print(\"DEBUG cat_cols:\", cat_cols)\n",
    "        print(\"DEBUG cont_cols:\", cont_cols)\n",
    "        print(\"DEBUG X_train columns:\", self.X_train.columns)\n",
    "        print(\"DEBUG X_train[cat_cols] dtype:\", self.X_train[cat_cols].values.dtype)\n",
    "\n",
    "        X_train_cat = torch.tensor(self.X_train[cat_cols].values, dtype=torch.long, device=self.device)\n",
    "        X_train_cont = torch.tensor(self.X_train[cont_cols].values, dtype=torch.float32, device=self.device)\n",
    "        y_train = torch.tensor(self.y_train.values, dtype=torch.float32, device=self.device)\n",
    "        X_test_cat = torch.tensor(self.X_test[cat_cols].values, dtype=torch.long, device=self.device)\n",
    "        X_test_cont = torch.tensor(self.X_test[cont_cols].values, dtype=torch.float32, device=self.device)\n",
    "        y_test = torch.tensor(self.y_test.values, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.train_dataset = TensorDataset(X_train_cat, X_train_cont, y_train)\n",
    "        self.val_dataset = TensorDataset(X_test_cat, X_test_cont, y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26202c4",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We use the SAINT architecture and Dataloader class to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec31e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG cat_cols: ['EVT']\n",
      "DEBUG cont_cols: ['EVH', 'NDVI', 'PRES_max', 'SPFH_max', 'TMP_max', 'WIND_max', 'elevation', 'sm_profile', 'sm_profile_wetness', 'sm_rootzone', 'sm_rootzone_wetness', 'sm_surface', 'sm_surface_wetness', 'year', 'month', 'day', 'longitude', 'latitude']\n",
      "DEBUG X_train columns: Index(['EVT', 'EVH', 'NDVI', 'PRES_max', 'SPFH_max', 'TMP_max', 'WIND_max',\n",
      "       'elevation', 'sm_profile', 'sm_profile_wetness', 'sm_rootzone',\n",
      "       'sm_rootzone_wetness', 'sm_surface', 'sm_surface_wetness', 'year',\n",
      "       'month', 'day', 'longitude', 'latitude'],\n",
      "      dtype='object')\n",
      "DEBUG X_train[cat_cols] dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | embeddings  | ModuleList         | 1.2 K  | train\n",
      "1 | cont_proj   | Linear             | 608    | train\n",
      "2 | transformer | TransformerEncoder | 38.1 K | train\n",
      "3 | classifier  | Sequential         | 1.1 K  | train\n",
      "4 | loss_fn     | BCEWithLogitsLoss  | 0      | train\n",
      "-----------------------------------------------------------\n",
      "41.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.1 K    Total params\n",
      "0.164     Total estimated model params size (MB)\n",
      "41        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a491083a034141baaff7657651b64014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Initialize model\n",
    "model = SAINT(\n",
    "    num_cont=len([col for col in X_train.columns if col not in cat_cols]),\n",
    "    cat_dims=cat_dims,\n",
    "    embed_dim=32,\n",
    "    num_heads=4,\n",
    "    num_layers=3\n",
    ")\n",
    "\n",
    "# Initialize DataModule\n",
    "dm = FireDataModule(X_train, y_train, X_test, y_test, cat_cols=cat_cols, batch_size=64)\n",
    "\n",
    "# Train with PyTorch Lightning using MPS (Mac GPU)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',  # This will use your Mac GPU!\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "        pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min')\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
